{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing : build a dataset for one cryptos with all information to be used by machine learning\n",
    "from utils_csa import show_model_accuracy, remove_outliers\n",
    "\n",
    "import numpy as np\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pytz\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil import parser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#import talib # https://github.com/mrjbq7/ta-lib    -    https://mrjbq7.github.io/ta-lib/\n",
    "#from talib.abstract import *\n",
    "from talib.abstract import *\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#from functools import reduce\n",
    "\n",
    "ohlcv_columns_to_be_cleaned = ['close_price', 'open_price', 'low_price', 'high_price', 'volume_aggregated_1h']\n",
    "str_sql = 'postgresql://dbuser:algocryptos@localhost:9091/algocryptos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== UTILS ========\n",
    "\n",
    "def do_timestamp_tasks(df_ts):\n",
    "    df_ts = df_ts[~df_ts.timestamp.duplicated(keep='first')]\n",
    "    df_ts['timestamp'] = pd.to_datetime(df_ts.timestamp, utc=True)\n",
    "    return df_ts.set_index('timestamp')\n",
    "\n",
    "def join_ohlcv_1h_1d(df_ohlcv_p, df_ohlcv_1d_p):\n",
    "    # drop columns that are in both dataframes\n",
    "    df_ohlcv_1d_p = df_ohlcv_1d_p.drop(['open_price', 'high_price', 'low_price', 'close_price', 'volume_aggregated_1h'], axis=1)\n",
    "    \n",
    "    # Interpolation ok (checked with ploting before / after each indicator)\n",
    "    df_ohlcv_p = df_ohlcv_p.join(df_ohlcv_1d_p.resample('1H').interpolate())    \n",
    "    return df_ohlcv_p\n",
    "\n",
    "\n",
    "# ======== RETRIEVE DATA FROM DB ========\n",
    "def get_dataset_ohlcv(connection, id_cryptocompare):  \n",
    "    squery = \"select oh.open_price, oh.high_price, oh.low_price, oh.close_price, oh.volume_aggregated as volume_aggregated_1h, oh.timestamp\\n\" #re.reddit_subscribers,\n",
    "    squery += 'from histo_ohlcv oh\\n'\n",
    "    squery += 'where oh.id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += 'order by oh.timestamp asc\\n'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_reddit(connection, id_cryptocompare):\n",
    "    squery = \"select re.reddit_subscribers, date_trunc('day', re.timestamp) + '00:00:00' as timestamp\\n\"\n",
    "    squery += 'from social_stats_reddit_histo re\\n'\n",
    "    squery += 'where re.reddit_subscribers <> 0 and re.id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += 'order by re.timestamp asc\\n'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_all_cryptos(connection):\n",
    "    squery = 'select sum(hi.close_price * hi.volume_aggregated) as global_volume_usd_1h, sum(hi.close_price * pr.available_supply) as global_market_cap_usd, hi.timestamp\\n'\n",
    "    squery += 'from histo_ohlcv hi\\n'\n",
    "    squery += 'inner join coins co on (hi.id_cryptocompare = co.id_cryptocompare)\\n'\n",
    "    squery += 'left outer join prices pr on (pr.id_cryptocompare = hi.id_cryptocompare)\\n'\n",
    "    squery += 'group by timestamp\\n'\n",
    "    squery += 'order by timestamp'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_google_trend(connection, id_cryptocompare, period):\n",
    "    squery = 'select value_standalone, value_compared_to_standard, timestamp\\n'\n",
    "    squery += 'from social_google_trend' + period + '\\n'\n",
    "    squery += 'where id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += 'order by timestamp'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_ohlcv_old(connection, id_cryptocompare, before_date):\n",
    "    squery = \"select oh.open_price, oh.high_price, oh.low_price, oh.close_price, oh.volume_usd as volume_aggregated_1h, oh.timestamp\\n\"\n",
    "    squery += 'from histo_ohlcv_old oh\\n'\n",
    "    squery += 'where oh.id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += \"and oh.timestamp < '\" + str(before_date) + \"'\\n\"\n",
    "    squery += 'order by oh.timestamp desc\\n'\n",
    "    squery += 'limit 60\\n'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_ids_top_n_cryptos(connection, top_n):\n",
    "    squery = 'select id_cryptocompare from prices where crypto_rank between 1 and ' + str(top_n) + '\\n'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "# ======== PROCESS / DATA ========\n",
    "def get_ohlcv_1d_plus_missing_infos(connection, df_ohlcv_p, id_cryptocompare):\n",
    "    # TODO : Perf : do only one call to these two lines (cf. get_ohlcv_1h_plus_missing_infos)\n",
    "    df_ohlcv_old = get_dataset_ohlcv_old(connection, id_cryptocompare, df_ohlcv_p.index.min())\n",
    "    \n",
    "    # resample to 1d\n",
    "    df_ohlcv_1d = df_ohlcv_p.resample(\"1D\").agg({'open_price': 'first', 'high_price': 'max', 'low_price': 'min', \n",
    "                                         'close_price': 'last', 'volume_aggregated_1h': 'sum'})\n",
    "    \n",
    "    df_final = df_ohlcv_1d\n",
    "    \n",
    "    # Only when datafarme contains rows\n",
    "    if len(df_ohlcv_old.index) > 0:\n",
    "        df_ohlcv_old = clean_dataset_ohlcv_std(df_ohlcv_old, ohlcv_columns_to_be_cleaned, resample='1D')        \n",
    "\n",
    "        # resample to 1d\n",
    "        df_ohlcv_old = df_ohlcv_old.resample(\"1D\").agg({'open_price': 'first', 'high_price': 'max', 'low_price': 'min', \n",
    "                                         'close_price': 'last', 'volume_aggregated_1h': 'sum'})\n",
    "\n",
    "        # quick & dirty way to have coherents volumes between both dataset\n",
    "        mean_vol_old = df_ohlcv_old.tail(5).volume_aggregated_1h.mean()\n",
    "        mean_vol_1d = df_ohlcv_1d.head(5).volume_aggregated_1h.mean()\n",
    "        df_ohlcv_old.volume_aggregated_1h = df_ohlcv_old.volume_aggregated_1h / (mean_vol_old / mean_vol_1d)\n",
    "        df_final = pd.concat([df_ohlcv_old, df_ohlcv_1d])\n",
    "\n",
    "        df_final = df_final[~df_final.index.duplicated()]\n",
    "    \n",
    "    # trick to allow to have data for indicators on last rows\n",
    "    df_last_row = df_ohlcv_p.tail(1).copy()\n",
    "    df_last_row.index = [pd.to_datetime(df_final.tail(1).index.values[0] + np.timedelta64(1,'D'), utc=True)]\n",
    "    \n",
    "    # extrapolate 24h vol from mean of last 6 hours\n",
    "    df_last_row.volume_aggregated_1h = df_ohlcv_p.tail(6).volume_aggregated_1h.mean() * 4\n",
    "    \n",
    "    df_final = df_final.append(df_last_row)\n",
    "    return df_final\n",
    "\n",
    "def get_ohlcv_1h_plus_missing_infos(connection, df_ohlcv_p, id_cryptocompare):\n",
    "    # get data older than 12/2017\n",
    "    df_ohlcv_old = get_dataset_ohlcv_old(connection, id_cryptocompare, df_ohlcv_p.index.min())\n",
    "    \n",
    "    df_final = df_ohlcv_p\n",
    "    \n",
    "    # Only when datafarme contains rows\n",
    "    if len(df_ohlcv_old.index) > 0:\n",
    "        df_ohlcv_old = clean_dataset_ohlcv_std(df_ohlcv_old, ohlcv_columns_to_be_cleaned, resample='1D')\n",
    "    \n",
    "        # resample to 1h\n",
    "        df_ohlcv_old = df_ohlcv_old.resample(\"1H\").interpolate()\n",
    "        df_ohlcv_old.volume_aggregated_1h = df_ohlcv_old.volume_aggregated_1h / 24\n",
    "    \n",
    "        # quick & dirty way to have coherents volumes between both dataset\n",
    "        mean_vol_old = df_ohlcv_old.tail(5).volume_aggregated_1h.mean()\n",
    "        mean_vol_ohlcv = df_ohlcv_p.head(5).volume_aggregated_1h.mean()\n",
    "        df_ohlcv_old.volume_aggregated_1h = df_ohlcv_old.volume_aggregated_1h / (mean_vol_old / mean_vol_ohlcv)\n",
    "        df_final = pd.concat([df_ohlcv_old, df_ohlcv_p])\n",
    "    \n",
    "    df_final = df_final[~df_final.index.duplicated()]\n",
    "    return df_final\n",
    "\n",
    "def merge_google_trend_data(df_google_trend_crypto_1m_p, df_google_trend_crypto_5y_p):\n",
    "    # put data on the same scale\n",
    "    first_row_1m = df_google_trend_crypto_1m_p.head(1)\n",
    "    equiv_row_5y = df_google_trend_crypto_5y_p.loc[first_row_1m.index.values[0]]\n",
    "\n",
    "    ratio_standalone = first_row_1m.value_standalone[0] / equiv_row_5y.value_standalone\n",
    "    ratio_compared_to_standard = first_row_1m.value_compared_to_standard[0] / equiv_row_5y.value_compared_to_standard\n",
    "\n",
    "    df_google_trend_crypto_1m_p.value_standalone = df_google_trend_crypto_1m_p.value_standalone / ratio_standalone\n",
    "    df_google_trend_crypto_1m_p.value_compared_to_standard = df_google_trend_crypto_1m_p.value_compared_to_standard / ratio_compared_to_standard\n",
    "\n",
    "    # replace data from 5y with more precise data from 1m    \n",
    "    start_remove = df_google_trend_crypto_1m_p.head(1).index.values[0]\n",
    "    end_remove = df_google_trend_crypto_1m_p.tail(1).index.values[0]          \n",
    "    \n",
    "    df_google_trend_crypto_5y_p = df_google_trend_crypto_5y_p.loc[(df_google_trend_crypto_5y_p.index.values < start_remove) | (df_google_trend_crypto_5y_p.index.values > end_remove)]\n",
    "    df_google_trend_crypto_5y_p = pd.concat([df_google_trend_crypto_5y_p, df_google_trend_crypto_1m_p])\n",
    "    \n",
    "    return df_google_trend_crypto_5y_p\n",
    "\n",
    "def clean_dataset_google_trend(df_google_trend_p):\n",
    "    df_google_trend_p = do_timestamp_tasks(df_google_trend_p)\n",
    "    df_google_trend_p = df_google_trend_p.resample('1H').interpolate()\n",
    "    df_google_trend_p['value_standalone'] = df_google_trend_p['value_standalone'].astype(int)\n",
    "    df_google_trend_p['value_compared_to_standard'] = df_google_trend_p['value_compared_to_standard'].astype(int)\n",
    "    \n",
    "    # avoid infinity values (bias not big)\n",
    "    df_google_trend_p.value_standalone = df_google_trend_p.value_standalone.replace(0, 1)\n",
    "    df_google_trend_p.value_compared_to_standard = df_google_trend_p.value_compared_to_standard.replace(0, 1)\n",
    "    return df_google_trend_p\n",
    "\n",
    "def clean_dataset_ohlcv_spe(df_ohlcv_p):\n",
    "    # drop rows with missing values (OHLCV)\n",
    "    df_ohlcv_p = df_ohlcv_p.loc[(df_ohlcv_p.open_price != 0.0) & (df_ohlcv_p.high_price != 0.0) & (df_ohlcv_p.low_price != 0.0) & (df_ohlcv_p.close_price != 0.0) & (df_ohlcv_p.volume_aggregated_1h != 0.0)]\n",
    "    return clean_dataset_ohlcv_std(df_ohlcv_p, ohlcv_columns_to_be_cleaned)\n",
    "\n",
    "def clean_dataset_ohlcv_std(df_ohlcv_p, columns_name, do_ts_tasks=True, resample='1H'):\n",
    "    # perform different tasks on df\n",
    "    if do_ts_tasks:\n",
    "        df_ohlcv_p = do_timestamp_tasks(df_ohlcv_p)\n",
    "    df_ohlcv_p = remove_outliers(df_ohlcv_p, columns_name)\n",
    "    \n",
    "    # no scale change (regarding calls done in code)\n",
    "    df_ohlcv_p = df_ohlcv_p.resample(resample).interpolate()  \n",
    "    return df_ohlcv_p\n",
    "\n",
    "# ======== FEATURE ENGINEERING ========\n",
    "def feature_engineering_ohlcv(df_ohlcv_p):\n",
    "    df_ohlcv_p = df_ohlcv_p.copy()\n",
    "    \n",
    "    # volume_aggregated_24h\n",
    "    df_ohlcv_p['volume_aggregated_24h'] = df_ohlcv_p.volume_aggregated_1h.rolling(24).sum()\n",
    "    \n",
    "    # close price variance on different scales\n",
    "    df_ohlcv_p['close_price_variance_3h'] = df_ohlcv_p.close_price.rolling(3).var()\n",
    "    df_ohlcv_p['close_price_variance_12h'] = df_ohlcv_p.close_price.rolling(12).var()\n",
    "    df_ohlcv_p['close_price_variance_24h'] = df_ohlcv_p.close_price.rolling(24).var()\n",
    "    df_ohlcv_p['close_price_variance_7d'] = df_ohlcv_p.close_price.rolling(7 * 24).var()\n",
    "    df_ohlcv_p['close_price_variance_15d'] = df_ohlcv_p.close_price.rolling(15 * 24).var()\n",
    "    df_ohlcv_p['close_price_variance_30d'] = df_ohlcv_p.close_price.rolling(30 * 24).var()\n",
    "    \n",
    "    # variance high / low on period\n",
    "    df_ohlcv_p['last_period_high_low_price_var_pct'] = abs(df_ohlcv_p['low_price'] - df_ohlcv_p['high_price']) / df_ohlcv_p['close_price']\n",
    "    \n",
    "    # volumes kpis 1h, 3h, 6h, 12h, 24h, 3d, 7d, 15d\n",
    "    df_ohlcv_p['mean_volume_1h_30d'] = df_ohlcv_p.volume_aggregated_1h / df_ohlcv_p.volume_aggregated_1h.rolling(30 * 24).mean()\n",
    "    arr_nums = [3, 6, 12, 24, 3 * 24, 7 * 24, 15 * 24]\n",
    "    arr_labels = ['3h', '6h', '12h', '24h', '3d', '7d', '15d']\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_ohlcv_p['mean_volume_' + arr_labels[i] + '_30d'] = df_ohlcv_p.volume_aggregated_1h.rolling(arr_nums[i]).mean() / df_ohlcv_p.volume_aggregated_1h.rolling(30 * 24).mean()\n",
    "    \n",
    "    # change vs n days low / n days high - pct_change for periods : 3d, 7d, 15d, 30d\n",
    "    arr_nums = np.array([3, 7, 15, 30], dtype=int) * 24\n",
    "    arr_labels = ['3d', '7d', '15d', '30d']\n",
    "    \n",
    "    # lows\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_ohlcv_p['close_price_pct_change_vs_' + arr_labels[i] + '_low'] = (df_ohlcv_p.close_price - df_ohlcv_p.close_price.rolling(arr_nums[i]).min()) / df_ohlcv_p.close_price.rolling(arr_nums[i]).min()      \n",
    "        \n",
    "    # highs\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_ohlcv_p['close_price_pct_change_vs_' + arr_labels[i] + '_high'] = (df_ohlcv_p.close_price - df_ohlcv_p.close_price.rolling(arr_nums[i]).max()) / df_ohlcv_p.close_price.rolling(arr_nums[i]).max()      \n",
    "    return df_ohlcv_p\n",
    "    \n",
    "def feature_engineering_ohlcv_all_cryptos(df_ohlcv_all_p):\n",
    "    # volume_aggregated_24h\n",
    "    df_ohlcv_all_p['global_volume_usd_24h'] = df_ohlcv_all_p.global_volume_usd_1h.rolling(24).sum()\n",
    "    return df_ohlcv_all_p\n",
    "\n",
    "def feature_engineering_reddit(df_reddit_p):    \n",
    "    # pct_change for periods : 1d, 3d, 7d, 15d, 30d\n",
    "    arr_nums = np.array([1, 3, 7, 15, 30], dtype=int) * 24\n",
    "    arr_labels = ['1d', '3d', '7d', '15d', '30d']\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_reddit_p['reddit_subscribers_pct_change_' + arr_labels[i]] = df_reddit_p.reddit_subscribers.pct_change(periods=arr_nums[i])\n",
    "    return df_reddit_p\n",
    "\n",
    "def feature_engineering_google_trend(df_google_trend_p, period):\n",
    "    # period = month\n",
    "    arr_nums = np.array([1, 3, 7, 15, 30], dtype=int) * 24\n",
    "    arr_labels = ['1d', '3d', '7d', '15d', '30d']\n",
    "    \n",
    "    #period = year\n",
    "    if period == 'y':\n",
    "        # pct_change for periods : 2m, 3m, 6m, 1y\n",
    "        arr_nums = np.array([2, 3, 6, 12], dtype=int) * 24 * 30\n",
    "        arr_labels = ['2m', '3m', '6m', '1y']   \n",
    "    \n",
    "    for i in range(len(arr_nums)):\n",
    "        df_google_trend_p['gg_trend_value_standalone_pct_change_' + arr_labels[i]] = df_google_trend_p.value_standalone.pct_change(periods=arr_nums[i])\n",
    "        df_google_trend_p['gg_trend_value_compared_pct_change_' + arr_labels[i]] = df_google_trend_p.value_compared_to_standard.pct_change(periods=arr_nums[i])\n",
    "    return df_google_trend_p\n",
    "\n",
    "def feature_engineering_technical_analysis(df_ohlcv_p, df_ohlcv_1d_p):\n",
    "    df_ohlcv_tmp = df_ohlcv_p.copy()\n",
    "    df_ohlcv_1d = df_ohlcv_1d_p.copy()\n",
    "    \n",
    "    # ========== INDICATORS CALCUL ==========\n",
    "\n",
    "    # [Overlap Studies] EMA 30 days\n",
    "    df_ohlcv_1d['Indic_EMA_30d'] = EMA(df_ohlcv_1d, price='close_price', timeperiod=30)    \n",
    "    # [Overlap Studies] EMA 15 days\n",
    "    df_ohlcv_1d['Indic_EMA_15d'] = EMA(df_ohlcv_1d, price='close_price', timeperiod=15)    \n",
    "    # [Overlap Studies] EMA 7 days\n",
    "    df_ohlcv_1d['Indic_EMA_7d'] = EMA(df_ohlcv_1d, price='close_price', timeperiod=7)\n",
    "    \n",
    "    # [Overlap Studies] MA 30 days\n",
    "    df_ohlcv_1d['Indic_MA_30d'] = MA(df_ohlcv_1d, price='close_price', timeperiod=30, matype=0)    \n",
    "    # [Overlap Studies] MA 15 days\n",
    "    df_ohlcv_1d['Indic_MA_15d'] = MA(df_ohlcv_1d, price='close_price', timeperiod=15, matype=0)    \n",
    "    # [Overlap Studies] MA 7 days\n",
    "    df_ohlcv_1d['Indic_MA_7d'] = MA(df_ohlcv_1d, price='close_price', timeperiod=7, matype=0)    \n",
    "\n",
    "    # [Overlap Studies] BBands - TODO : 20 days ?\n",
    "    bands = BBANDS(df_ohlcv_1d, price='close_price', timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    bands.columns = ['Indic_Bbands_20d_upperband', 'Indic_Bbands_20d_middleband', 'Indic_Bbands_20d_lowerband']\n",
    "    df_ohlcv_1d = df_ohlcv_1d.join(bands)    \n",
    "\n",
    "    # [Momentum Indicator] RSI 14 days\n",
    "    df_ohlcv_1d['Indic_RSI_14d'] = RSI(df_ohlcv_1d, price='close_price', timeperiod=14)    \n",
    "\n",
    "    # [Momentum Indicators] STOCH\n",
    "    # ta-lib abstract API KO with dataframe : use workaround\n",
    "    dataset = {'high': df_ohlcv_1d.high_price.values, 'low': df_ohlcv_1d.low_price.values, 'close': df_ohlcv_1d.close_price.values}\n",
    "    kd = STOCH(dataset, fastk_period=14, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "    df_ohlcv_1d['Indic_Stoch_14_3_3_k'] = kd[0]\n",
    "    df_ohlcv_1d['Indic_Stoch_14_3_3_d'] = kd[1]\n",
    "\n",
    "    # [Momentum Indicators] MACD\n",
    "    macd = MACD(df_ohlcv_1d, price='close_price', fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    macd.columns = ['Indic_Macd_12_26_9_macd', 'Indic_Macd_12_26_9_macdsignal', 'Indic_Macd_12_26_9_macdhist']\n",
    "    df_ohlcv_1d = df_ohlcv_1d.join(macd)\n",
    "\n",
    "    # [Volume Indicators] OBV\n",
    "    dataset = {'close': df_ohlcv_1d.close_price.values, 'volume': df_ohlcv_1d.volume_aggregated_1h.values}\n",
    "    obv = OBV(dataset)\n",
    "    df_ohlcv_1d['Indic_OBV'] = obv\n",
    "    \n",
    "    # join dataframes on 1h scale\n",
    "    df_ohlcv_tmp = join_ohlcv_1h_1d(df_ohlcv_tmp, df_ohlcv_1d)    \n",
    "    \n",
    "    # ========== ADD FEATURES FOR INTERPRETATION ==========\n",
    "    \n",
    "    # [Interpretation] EMA 30 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_EMA_30d_uptrend'] = (df_ohlcv_tmp.Indic_EMA_30d.pct_change(periods=1) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] EMA 15 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_EMA_15d_uptrend'] = (df_ohlcv_tmp.Indic_EMA_15d.pct_change(periods=1) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] EMA 7 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_EMA_7d_uptrend'] = (df_ohlcv_tmp.Indic_EMA_7d.pct_change(periods=1) > 0).astype(int).astype(float)\n",
    "    \n",
    "    # [Interpretation] MA 30 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_MA_30d_uptrend'] = (df_ohlcv_tmp.Indic_MA_30d.pct_change(periods=1) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] MA 15 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_MA_15d_uptrend'] = (df_ohlcv_tmp.Indic_MA_15d.pct_change(periods=1) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] MA 7 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_MA_7d_uptrend'] = (df_ohlcv_tmp.Indic_MA_7d.pct_change(periods=1) > 0).astype(int).astype(float)\n",
    "    \n",
    "    # [Interpretation] BBands close_price - Indic_Bbands_20d_upperband\n",
    "    df_ohlcv_tmp['Indic_Bbands_20d_diff_close_upperband'] = df_ohlcv_tmp.close_price - df_ohlcv_tmp.Indic_Bbands_20d_upperband\n",
    "    # [Interpretation] BBands close_price - Indic_Bbands_20d_middleband\n",
    "    df_ohlcv_tmp['Indic_Bbands_20d_diff_close_upperband'] = df_ohlcv_tmp.close_price - df_ohlcv_tmp.Indic_Bbands_20d_middleband\n",
    "    # [Interpretation] BBands close_price - Indic_Bbands_20d_middleband\n",
    "    df_ohlcv_tmp['Indic_Bbands_20d_diff_close_lowerband'] = df_ohlcv_tmp.close_price - df_ohlcv_tmp.Indic_Bbands_20d_lowerband\n",
    "    \n",
    "    # [Interpretation] RSI 14 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_RSI_14d_uptrend'] = (df_ohlcv_tmp.Indic_RSI_14d.pct_change(periods=1) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] RSI 14 days > value 70\n",
    "    df_ohlcv_tmp['Indic_RSI_sup_70'] = (df_ohlcv_tmp.Indic_RSI_14d > 70).astype(int).astype(float)\n",
    "    # [Interpretation] RSI 14 days < value 30\n",
    "    df_ohlcv_tmp['Indic_RSI_inf_30'] = (df_ohlcv_tmp.Indic_RSI_14d < 30).astype(int).astype(float)\n",
    "    \n",
    "    # [Interpretation] STOCH > value 80\n",
    "    df_ohlcv_tmp['Indic_Stoch_14_3_3_sup_80'] = ((df_ohlcv_tmp.Indic_Stoch_14_3_3_k > 80) & (df_ohlcv_tmp.Indic_Stoch_14_3_3_d > 80)).astype(int).astype(float)\n",
    "    # [Interpretation] STOCH < value 20\n",
    "    df_ohlcv_tmp['Indic_Stoch_14_3_3_inf_20'] = ((df_ohlcv_tmp.Indic_Stoch_14_3_3_k < 20) & (df_ohlcv_tmp.Indic_Stoch_14_3_3_d < 20)).astype(int).astype(float)\n",
    "    # [Interpretation] STOCH diff\n",
    "    df_ohlcv_tmp['Indic_Stoch_14_3_3_diff'] = df_ohlcv_tmp.Indic_Stoch_14_3_3_k - df_ohlcv_tmp.Indic_Stoch_14_3_3_d\n",
    "    \n",
    "    # [Interpretation] OBV in uptrend on last 3d : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_OBV_uptrend_3d'] = (df_ohlcv_tmp.Indic_OBV.pct_change(periods=3 * 24) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] OBV in uptrend on last 7d : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_OBV_uptrend_7d'] = (df_ohlcv_tmp.Indic_OBV.pct_change(periods=7 * 24) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] OBV in uptrend on last 15d : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_OBV_uptrend_15d'] = (df_ohlcv_tmp.Indic_OBV.pct_change(periods=15 * 24) > 0).astype(int).astype(float)\n",
    "    # [Interpretation] OBV in uptrend on last 30d : True / downtrend : False\n",
    "    df_ohlcv_tmp['Indic_OBV_uptrend_30d'] = (df_ohlcv_tmp.Indic_OBV.pct_change(periods=30 * 24) > 0).astype(int).astype(float)\n",
    "    \n",
    "    return df_ohlcv_tmp.drop(['open_price', 'high_price', 'low_price', 'close_price', 'volume_aggregated_1h'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_dataset_for_crypto(id_cryptocompare_crypto):    \n",
    "    # ------------------ PRE-PROCESSING : Data retrieving & cleaning ------------------ #\n",
    "\n",
    "    # TODO : Replace with info from config file\n",
    "    connection = create_engine(str_sql)\n",
    "\n",
    "    # Crypto ids\n",
    "    id_cryptocompare_crypto = str(id_cryptocompare_crypto)\n",
    "    id_cryptocompare_tether = \"171986\"\n",
    "    id_cryptocompare_bitcoin = \"1182\"\n",
    "\n",
    "    # --------------------------------\n",
    "    # OHLCV\n",
    "    # --------------------------------\n",
    "    df_ohlcv = get_dataset_ohlcv(connection, id_cryptocompare_crypto)\n",
    "    df_ohlcv = clean_dataset_ohlcv_spe(df_ohlcv)\n",
    "    min_date = df_ohlcv.index.min()\n",
    "\n",
    "    df_ohlcv = get_ohlcv_1h_plus_missing_infos(connection, df_ohlcv, id_cryptocompare_crypto)\n",
    "\n",
    "    df_ohlcv_tether = get_dataset_ohlcv(connection, id_cryptocompare_tether)\n",
    "    df_ohlcv_tether = clean_dataset_ohlcv_spe(df_ohlcv_tether)\n",
    "    df_ohlcv_tether = get_ohlcv_1h_plus_missing_infos(connection, df_ohlcv_tether, id_cryptocompare_tether)\n",
    "\n",
    "    df_ohlcv_bitcoin = get_dataset_ohlcv(connection, id_cryptocompare_bitcoin)\n",
    "    df_ohlcv_bitcoin = clean_dataset_ohlcv_spe(df_ohlcv_bitcoin)\n",
    "    df_ohlcv_bitcoin = get_ohlcv_1h_plus_missing_infos(connection, df_ohlcv_bitcoin, id_cryptocompare_bitcoin)\n",
    "\n",
    "    df_ohlcv_1d = get_ohlcv_1d_plus_missing_infos(connection, df_ohlcv, id_cryptocompare_crypto)\n",
    "\n",
    "    # TODO : could be used later if we want TA on bitcoin \n",
    "    #df_ohlcv_1d_tether = get_ohlcv_1d_plus_missing_infos(connection, df_ohlcv_tether, id_cryptocompare_tether)\n",
    "    # could be used to do technical analysis also\n",
    "    #df_ohlcv_1d_bitcoin = get_ohlcv_1d_plus_missing_infos(connection, df_ohlcv_bitcoin, id_cryptocompare_bitcoin)\n",
    "\n",
    "    # --------------------------------\n",
    "    # REDDIT SUBSCRIBERS\n",
    "    # --------------------------------\n",
    "    df_reddit = get_dataset_reddit(connection, id_cryptocompare_crypto)\n",
    "    df_reddit = df_reddit[df_reddit.reddit_subscribers.notnull()]\n",
    "    df_reddit = do_timestamp_tasks(df_reddit)\n",
    "    df_reddit = df_reddit.resample('1H').interpolate()\n",
    "    df_reddit['reddit_subscribers'] = df_reddit['reddit_subscribers'].astype(int)\n",
    "\n",
    "    # --------------------------------\n",
    "    # ALL CRYPTOS\n",
    "    # --------------------------------\n",
    "    df_all_cryptos = get_dataset_all_cryptos(connection)\n",
    "    df_all_cryptos = clean_dataset_ohlcv_std(df_all_cryptos, columns_name=['global_volume_usd_1h', 'global_market_cap_usd'])\n",
    "\n",
    "    # --------------------------------\n",
    "    # GOOGLE TREND\n",
    "    # --------------------------------\n",
    "    # crypto - last month => Need to import and keep old data\n",
    "    df_google_trend_crypto_1m = get_dataset_google_trend(connection, id_cryptocompare_crypto, '_1m')\n",
    "    df_google_trend_crypto_1m = clean_dataset_google_trend(df_google_trend_crypto_1m)\n",
    "\n",
    "    # crypto - 5 years\n",
    "    df_google_trend_crypto_5y = get_dataset_google_trend(connection, id_cryptocompare_crypto, '')\n",
    "    df_google_trend_crypto_5y = clean_dataset_google_trend(df_google_trend_crypto_5y)\n",
    "\n",
    "    # bitcoin - last month\n",
    "    df_google_trend_bitcoin_1m = get_dataset_google_trend(connection, id_cryptocompare_bitcoin, '_1m')\n",
    "    df_google_trend_bitcoin_1m = clean_dataset_google_trend(df_google_trend_bitcoin_1m)\n",
    "\n",
    "    # bitcoin - 5 years\n",
    "    df_google_trend_bitcoin_5y = get_dataset_google_trend(connection, id_cryptocompare_bitcoin, '')\n",
    "    df_google_trend_bitcoin_5y = clean_dataset_google_trend(df_google_trend_bitcoin_5y)\n",
    "\n",
    "    # merge data\n",
    "    df_google_trend_crypto_5y = merge_google_trend_data(df_google_trend_crypto_1m, df_google_trend_crypto_5y)\n",
    "    df_google_trend_bitcoin_5y = merge_google_trend_data(df_google_trend_bitcoin_1m, df_google_trend_bitcoin_5y)\n",
    "    \n",
    "    # ------------------ PRE-PROCESSING : Feature engineering ------------------ #\n",
    "    df_reddit = feature_engineering_reddit(df_reddit)\n",
    "    df_ohlcv_fe = feature_engineering_ohlcv(df_ohlcv)\n",
    "    df_ohlcv_tether_fe = feature_engineering_ohlcv(df_ohlcv_tether)\n",
    "    df_ohlcv_bitcoin_fe = feature_engineering_ohlcv(df_ohlcv_bitcoin)\n",
    "    df_technical_analysis = feature_engineering_technical_analysis(df_ohlcv, df_ohlcv_1d)\n",
    "    df_all_cryptos = feature_engineering_ohlcv_all_cryptos(df_all_cryptos)\n",
    "    df_google_trend_crypto_5y = feature_engineering_google_trend(df_google_trend_crypto_5y, 'y')\n",
    "    df_google_trend_bitcoin_5y = feature_engineering_google_trend(df_google_trend_bitcoin_5y, 'y')\n",
    "\n",
    "    # Join dfs\n",
    "    df_ohlcv_fe = df_ohlcv_fe.join(df_ohlcv_tether_fe, rsuffix='_tether')\n",
    "    df_ohlcv_fe = df_ohlcv_fe.join(df_ohlcv_bitcoin_fe, rsuffix='_bitcoin')\n",
    "\n",
    "    df_global = df_ohlcv_fe.join(df_technical_analysis)\n",
    "    df_global = df_global.join(df_reddit)\n",
    "    df_global = df_global.join(df_all_cryptos)\n",
    "    df_global = df_global.join(df_google_trend_crypto_5y, rsuffix='_crypto_5y')\n",
    "    df_global = df_global.join(df_google_trend_bitcoin_5y, rsuffix='_bitcoin_5y')\n",
    "    df_global.resample('1H').interpolate()\n",
    "    df_global.reddit_subscribers = df_global.reddit_subscribers.interpolate(method='linear', limit_area='outside')\n",
    "\n",
    "    # remove data added only to be able to calcul indicators, etc. => we don't want to take it into account\n",
    "    df_global = df_global[min_date:df_global.index.max()]\n",
    "\n",
    "    # remove 24 first hours (some things can't be extrapolated well)\n",
    "    df_global = df_global.iloc[24:]\n",
    "    df_global= df_global.interpolate(method='nearest', axis=0).ffill()\n",
    "    \n",
    "    # drop na if exist\n",
    "    df_final = df_global.dropna(axis='rows')\n",
    "    diff = df_global.shape[0] - df_final.shape[0]\n",
    "    if(diff > 0):\n",
    "        print(str(diff) + ' rows containing Nan dropped')\n",
    "    \n",
    "    return df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_datasets_for_cryptos(ids_cryptocompare_crypto):\n",
    "    dict_df = {}\n",
    "    for id_crypto in ids_cryptocompare_crypto:\n",
    "        print('Crypto : ' + str(id_crypto))\n",
    "        try:\n",
    "            df = get_global_dataset_for_crypto(id_crypto)\n",
    "            if df.empty:\n",
    "                print('ALERT : Empty dataframe')\n",
    "            else:\n",
    "                dict_df[str(id_crypto)] = df            \n",
    "        except:\n",
    "            print('ERROR : get_global_dataset_for_crypto() for crypto : ' + str(id_crypto))\n",
    "\n",
    "    return dict_df\n",
    "\n",
    "def get_global_datasets_for_top_n_cryptos(top_n=20):\n",
    "    connection_tmp = create_engine(str_sql)\n",
    "    df = get_dataset_ids_top_n_cryptos(connection_tmp, top_n)    \n",
    "    return get_global_datasets_for_cryptos(df.id_cryptocompare.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ PRE-PROCESSING : Calcul y + split data ------------------ #\n",
    "def calcul_values_of_y(df, dict_hours_labels, increase_target_pct):\n",
    "        increase_target_pct = increase_target_pct / 100\n",
    "\n",
    "        for key in dict_hours_labels:\n",
    "            label_value = 'y_+' + dict_hours_labels[key] + '_value'\n",
    "            label_classif = 'y_+' + dict_hours_labels[key] + '_classif'\n",
    "            # calcul several y searched (value)\n",
    "            df[label_value] = df.close_price.shift(-key)\n",
    "\n",
    "            # perform calcul to use binary classification\n",
    "            if increase_target_pct > 0:\n",
    "                df[label_classif] = ((df[label_value] - df['close_price']) / df['close_price']) >= increase_target_pct\n",
    "            else:\n",
    "                df[label_classif] = ((df[label_value] - df['close_price']) / df['close_price']) <= increase_target_pct\n",
    "\n",
    "        return df\n",
    "\n",
    "def do_split_data(df_p, columns_nb_p):\n",
    "    # separe x,y\n",
    "    X = df_p.iloc[:,range(1, columns_nb_p)]\n",
    "    y = df_p.iloc[:,range(columns_nb_p, len(df_p.columns))]\n",
    "\n",
    "    # split data in training / validating / testing\n",
    "    return train_test_split(X, y, random_state=0, shuffle=False)\n",
    "\n",
    "def get_preprocessed_data(dict_df, dict_hours_labels, close_price_increase_targeted, predict_only_one_crypto,\n",
    "                         do_scale=True, do_pca=False, id_cryptocompare=0):\n",
    "    columns_nb = 0\n",
    "    df_new_dict = {}\n",
    "    df_new_list = []    \n",
    "\n",
    "    # calcul y for each crypto\n",
    "    for key_id_cryptocompare, df_one_crypto in dict_df.items():\n",
    "        # number of columns before adding y values - could be done once only\n",
    "        columns_nb = len(df_one_crypto.columns)\n",
    "\n",
    "        # calcul all y values we are interested in and add it to the dataframe\n",
    "        df_one_crypto = calcul_values_of_y(df_one_crypto.copy(), dict_hours_labels, close_price_increase_targeted)\n",
    "\n",
    "        # remove rows where y can't be calculed (need more data in the future)\n",
    "        df_one_crypto.dropna(subset=list(df_one_crypto.iloc[:,range(columns_nb, len(df_one_crypto.columns))]), inplace=True)\n",
    "\n",
    "        df_new_dict[key_id_cryptocompare] = df_one_crypto\n",
    "        df_new_list.append(df_one_crypto)\n",
    "\n",
    "    # concat to get only one dataframe instead of a list of dataframes\n",
    "    df_global = pd.concat(df_new_list).sort_index()\n",
    "    df_global.reset_index(drop=True)\n",
    "\n",
    "    # All cryptos\n",
    "    X_train, X_test, y_train, y_test = do_split_data(df_global, columns_nb)\n",
    "\n",
    "    if predict_only_one_crypto:\n",
    "        # The one to predict\n",
    "        X_train_one_crypto, X_test_one_crypto, y_train_one_crypto, y_test_one_crypto = do_split_data(df_new_dict[id_cryptocompare], columns_nb)\n",
    "        X_test = X_test_one_crypto\n",
    "        y_test = y_test_one_crypto\n",
    "\n",
    "    # TODO : To be used to avoid overfitting : No tuning while using testing data, only validation\n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "    # ------------------ PRE-PROCESSING ------------------ #\n",
    "    \n",
    "    X_train_close_price = X_train.close_price\n",
    "    X_test_close_price = X_test.close_price\n",
    "    \n",
    "    # Scaling Data\n",
    "    if do_scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # PCA to reduce dimensionality\n",
    "    if do_pca:\n",
    "        pca = PCA(n_components=35) # approx 97% variance\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test, X_train_close_price.values, X_test_close_price.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
