{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL : What am I searching for ?\n",
    "        # => To predict with the best possible accuracy (proba) an event (+/- xx% variation of close_price)\n",
    "        # What for ?\n",
    "            # => To be able to balance my portfolio (sell what should go down for what should go up)\n",
    "            # => Sell crypto for Stable coin / Or buy crypto with Stable Coin\n",
    "########\n",
    "\n",
    "# Advices : \n",
    "    # Be careful to scaling, min / max can change in the future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE\n",
    "    # 1 / Industrialization : \n",
    "        # Dataset for TOP 20 cryptos + capacity to learn from top 20 and predict 1 : DONE !\n",
    "    # 2 / Gérer le fait qu'on a des datasets déséquilibrés ? : OK\n",
    "    # 3 / Gérer l'overfitting de ouf : OK\n",
    "    # 4 / Play with Treshold\n",
    "    # 4 bis / \n",
    "        # => ROC Curve, learning curve & precision / recall curve au top et bien comprendre !       \n",
    "            # => Lire tout : https://classeval.wordpress.com/introduction/introduction-to-the-roc-receiver-operating-characteristics-plot/\n",
    "        # => cf. schéma avec 2 lignes de Treshold. Le treshold ça marche comment ? Il est ou sur precision ou recall ? \n",
    "        # => Est-ce que ça a un sens ?? Bien comprendre !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Now :\n",
    "    \n",
    "    # 5 / Coder la systématisation :\n",
    "        # Valeurs par défaut pour algo, checker un peu ce qu'il faut pour éviter overfitting (dépend nb crytpos !)\n",
    "        # df_logging : ajout metrics + trucs sérialisé pour pouvoir afficher graph ?\n",
    "        \n",
    "    # 5 bis : Unbalanced : downsampling majority class or upsampling minority class ! cf. livre ML finance\n",
    "        # Est-ce nécessaire pour améliorer l'apprentissage ou juste pour analyser la perf de l'algo ?\n",
    "        # Sinon utiliser le param \"balanced\" => ex. Random forest\n",
    "        # https://elitedatascience.com/imbalanced-classes\n",
    "           \n",
    "    # 5 ter / Faire du gridsearch puis optimiser avec treshold\n",
    "        # Jouer un peu avec les algo et gridsearch pour bien comprendre (paramètres, fonctionnement, etc.)\n",
    "                # => Comment fonctionne l'amélioration via Precision score, etc. ? Tester + comprendre\n",
    "         # cf. schéma systematisation + prendre en compte Treshold\n",
    "                # => Evaluer selon différents Treshold : https://classeval.wordpress.com/introduction/model-wide-evaluation/\n",
    "        # Trouver un moyen pour que le ranking des modèles créés par GridSearch soit estimé en fonction de :\n",
    "        # precision + pas d'overfitting (scores train / test à peu près identiques)\n",
    "    \n",
    "    \n",
    "    # 5 quater / \n",
    "        # Switcher vers coinmarketcap api pro free tiers ? https://pro.coinmarketcap.com/pricing\n",
    "        # Sécuriser (si erreur api etc. => backup data des tables, etc.)\n",
    "                \n",
    "    \n",
    "    # 6 / Construire algo de strategy backtest Buy / Sell + indicateur de stratégie (prendre en compte prévisions d'augmentation et prévisions de baisse)  \n",
    "    \n",
    "    # 7 / Une fois que j'ai fait tout ça :\n",
    "        # lecture + recherche internet d'inspiration / travaux semblables (cf. livres)\n",
    "        # Feature engineering :\n",
    "            # New :\n",
    "                # Variance close price, volumes 3h, 6h, 12h, 24h, etc., more indicators on different scale\n",
    "                # Mean 24h price\n",
    "                # Mean 24h price - current price\n",
    "            # Delete useless ? Do real feature engineering with data vizualisation, correlation, etc.\n",
    "            # Impact feature engineering (correlation, suppression feature useless, etc.) cf. algocryptos_tests\n",
    "    \n",
    "    # 8 / Others :\n",
    "        # Serialize scikit learn model: http://scikit-learn.org/stable/modules/model_persistence.html\n",
    "        # Use cross validation ?\n",
    "        # Différents algo de scaling en preprocessing\n",
    "        # Mieux comprendre ce qui est fait par l'algo, ex Randomforest, afficher arbre avec seuils, etc.\n",
    "        # Voir si certaines cryptos quand rajouter pour learning foutent la merde\n",
    "        \n",
    "    # 9 / A terme je pourrais avoir :\n",
    "        # Des modèles qui se mettent à jour automatiquement\n",
    "        # Pleins de modèles utilisés à différentes échelles pour savoir si il faut vendre ou acheter ! \n",
    "            # cf. que faire en fonction des différentes probas\n",
    "        # Show learning curve ? Precision / Recall curve Other metrics ? Jouer sur Treshold pour maximiser précision\n",
    "        # Deploy Model in AWS (serverless, cf. tuto https://medium.com/@patrickmichelberger/how-to-deploy-a-serverless-machine-learning-microservice-with-aws-lambda-aws-api-gateway-and-d5b8cbead846\n",
    "        # Use AWS for model fitting ?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "#Image(filename='Schema.jpg')\n",
    "#Image(filename='Systematisation.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from algocryptos_preprocessing.ipynb\n",
      "importing Jupyter notebook from algocryptos_gridsearch.ipynb\n"
     ]
    }
   ],
   "source": [
    "from utils_csa import show_model_accuracy, save_obj, load_obj, evaluate_model_formated\n",
    "\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Lib to calcul financial indicators https://github.com/mrjbq7/ta-lib\n",
    "import talib\n",
    "\n",
    "# Lib to import ipynb : https://pypi.org/project/import-ipynb/\n",
    "import import_ipynb\n",
    "import algocryptos_preprocessing as alg_preproc\n",
    "import algocryptos_gridsearch as alg_gridsearch\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "#pd.set_option('display.max_rows', 150)\n",
    "#pd.set_option('display.max_columns', 150)\n",
    "\n",
    "#np.set_printoptions(threshold='nan')\n",
    "dict_hours_labels  = {3:'3h', 6:'6h', 12:'12h', 24:'1d', 24*2:'2d', 24*3:'3d', 24*7:'7d', 24*15:'15d', 24*30:'30d'}\n",
    "\n",
    "str_sql = 'postgresql://dbuser:algocryptos@localhost:9091/algocryptos'\n",
    "connection = create_engine(str_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ PARAMETERS for SYSTEMATIZATION ------------------ #\n",
    "\n",
    "# +3h  : (+/-) 2.5 / 5% / 7.5% / 10%\n",
    "# +6h  : (+/-) 2.5 / 5% / 7.5% / 10%\n",
    "# +12h : (+/-) 5% / 7.5% / 10%\n",
    "# +1d  : (+/-) 5% / 7.5% / 10%\n",
    "# +2d  : (+/-) 5% / 7.5% / 10 / 15%\n",
    "# +3d  : (+/-) 5% / 10% / 15% / 20%\n",
    "# +7d  : (+/-) 10% / 20% / 30%\n",
    "# +15d : (+/-) 10% / 20% / 30% / 50%\n",
    "# +30d : (+/-) 10% / 20% / 30% / 50% / 100%\n",
    "\n",
    "# dynamic params\n",
    "param_datasets_nb_cryptos = [5, 10, 20, 30, 100] # 100 = max\n",
    "params_y = {'3h':[2.5, 5, 7.5, 10],\n",
    "            '6h':[2.5, 5, 7.5, 10],\n",
    "            '12h':[5, 7.5, 10],\n",
    "            '1d':[5, 7.5, 10],\n",
    "            '2d':[10, 15, 20],\n",
    "            '3d':[10, 20, 30],\n",
    "            '7d':[10, 20, 30, 50],\n",
    "            '15d':[10, 20, 30, 50, 100],\n",
    "            '30d':[10, 20, 30, 50, 100, 200]}\n",
    "\n",
    "# cf. XLS\n",
    "param_crypto_learning = [1182, 7605, 5031, 202330, 4614, 166503, 3808, 321992, 5038, 310829, 127356, 3807, 204788, 27368, 5324, \n",
    "                   5285, 166390, 24854, 236131, 41192, 347235,187440, 186277, 16713, 4432, 112392, 808414, 19745, 107672, \n",
    "                   716725, 324068, 5039, 5280, 172091, 309621, 4430, 13072, 20333, 4433, 20131, 33022, 17778, 24294, 890645]\n",
    "\n",
    "param_crypto_predicting = [1182, 7605, 5031, 4614, 166503, 3808, 321992, 5038, 310829, 127356, 3807, 204788, 27368, 5324, \n",
    "                     5285, 166390, 236131, 41192, 187440, 186277, 112392, 808414, 19745, 107672, 716725, 324068, 172091, \n",
    "                     309621, 13072, 20333, 20131, 33022, 17778, 24294, 890645]\n",
    "\n",
    "tresholds = [0.5, 0.7, 0.8, 0.9]\n",
    "\n",
    "# ['y_+3h_value', 'y_+3h_classif',, 'y_+6h_value', 'y_+6h_classif', 'y_+12h_value', 'y_+12h_classif','y_+1d_value', 'y_+1d_classif', 'y_+2d_value', 'y_+2d_classif', 'y_+3d_value', 'y_+3d_classif', 'y_+7d_value', 'y_+7d_classif', 'y_+15d_value', 'y_+15d_classif', 'y_+30d_value', 'y_+30d_classif']\n",
    "predict_only_one_crypto = False\n",
    "\n",
    "do_scale = True\n",
    "do_pca = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crypto : 1182\n",
      "Crypto : 7605\n",
      "Crypto : 5031\n",
      "Crypto : 202330\n",
      "Crypto : 4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/cysacenda/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 8 elements, new values have 20 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-04cb1fedf21a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                               \u001b[0;34m'param_Algo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'param_do_scale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'param_do_pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confusion_TN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confusion_FP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confusion_FN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                               \u001b[0;34m'confusion_TP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'precision_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recall_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_False'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                               'feat_importance_1', 'feat_importance_2', 'feat_importance_3']\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mdf_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logging_trt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   4387\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4388\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4389\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4390\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m   3321\u001b[0m             raise ValueError(\n\u001b[1;32m   3322\u001b[0m                 \u001b[0;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3323\u001b[0;31m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[1;32m   3324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3325\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 8 elements, new values have 20 elements"
     ]
    }
   ],
   "source": [
    "def get_XGBClassifier():\n",
    "    return XGBClassifier()\n",
    "\n",
    "def get_MLPClassifier():\n",
    "    return MLPClassifier(hidden_layer_sizes = [130, 130], solver='lbfgs',\n",
    "                     random_state = 0)\n",
    "\n",
    "def get_RandomForestClassifier():\n",
    "    return RandomForestClassifier(random_state=0, class_weight=\"balanced\", max_depth=5)\n",
    "\n",
    "\n",
    "# loop on different dataset size with different number of cryptos for learning (cf. params)\n",
    "for nb_cryptos in param_datasets_nb_cryptos:\n",
    "    # get n first cryptos ids\n",
    "    ids_cryptos = param_crypto_learning[:nb_cryptos]\n",
    "    dict_df = alg_preproc.get_global_datasets_for_cryptos(ids_cryptos)\n",
    "    \n",
    "    # loop on different y (prediction in future +3h, +6h, +1d, ect.)\n",
    "    for term, arr_pct_change in params_y.items():\n",
    "        df_logging = pd.DataFrame()\n",
    "        y_to_be_considered = 'y_+XXX_classif'.replace('XXX', term)\n",
    "        # loop on different price change in percentage for target\n",
    "        for close_price_targeted in arr_pct_change:            \n",
    "            # do +/- (increase or decrease close price %)\n",
    "            for factor in [-1, +1]:\n",
    "                X_train, X_test, y_train, y_test = alg_preproc.get_preprocessed_data(dict_df, dict_hours_labels, \n",
    "                                                                                 close_price_targeted * factor, \n",
    "                                                                                 predict_only_one_crypto,\n",
    "                                                                                 do_scale=do_scale, \n",
    "                                                                                 do_pca=do_pca)\n",
    "                \n",
    "                # Take into account param_crypto_predicting => Faire 1 seule à la fois chacune son tour puis tout ?\n",
    "                \n",
    "                # For each algo, find good base params \n",
    "                for clf_name, clf in {'XGBClassifier':get_XGBClassifier(), 'MLPClassifier':get_MLPClassifier(), \n",
    "                                      'RandomForestClassifier':get_RandomForestClassifier()}.items():\n",
    "                    try:\n",
    "                        clf_trained = clf.fit(X_train, y_train[y_to_be_considered])\n",
    "                        \n",
    "                        for treshold in tresholds:\n",
    "                            # logging\n",
    "                            log_values = pd.Series([datetime.now(), 'Success', len(ids_cryptos), term, close_price_targeted * factor, clf_name, do_scale, do_pca, evaluate_model_formated(clf, X_train, y_train[y_to_be_considered].values, threshold=threshold)])\n",
    "                            df_logging = df_logging.append(log_values, ignore_index=True)\n",
    "                        \n",
    "                    except:\n",
    "                        # logging\n",
    "                        log_values = pd.Series([datetime.now(), 'Error', len(ids_cryptos), term, close_price_targeted * factor, clf_name, do_scale, do_pca,\n",
    "                                               None, None,None, None,None, None,None, None,None, None,None, None,])\n",
    "                        df_logging = df_logging.append(log_values, ignore_index=True)\n",
    "                \n",
    "        # insert data into database\n",
    "        df_logging.columns = ['timestamp', 'status', 'param_crypto_learning', 'param_term', 'param_close_price_targeted', \n",
    "                              'param_Algo', 'param_do_scale', 'param_do_pca', 'confusion_TN', 'confusion_FP', 'confusion_FN',\n",
    "                              'confusion_TP', 'precision_score', 'recall_score', 'f1_score', 'support_True', 'support_False',\n",
    "                              'feat_importance_1', 'feat_importance_2', 'feat_importance_3']\n",
    "        df_logging.to_sql(name='logging_trt', con=connection, if_exists = 'append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ PARAMETERS ------------------ #\n",
    "# ['y_+3h_value', 'y_+3h_classif',, 'y_+6h_value', 'y_+6h_classif', 'y_+12h_value', 'y_+12h_classif','y_+1d_value', 'y_+1d_classif', 'y_+2d_value', 'y_+2d_classif', 'y_+3d_value', 'y_+3d_classif', 'y_+7d_value', 'y_+7d_classif', 'y_+15d_value', 'y_+15d_classif', 'y_+30d_value', 'y_+30d_classif']\n",
    "id_cryptocompare = \"7605\" # ether 7605\n",
    "predict_only_one_crypto = False\n",
    "y_to_be_considered = 'y_+1d_classif' #1d_classif\n",
    "close_price_increase_targeted = +5\n",
    "threshold = 0.5\n",
    "dict_hours_labels  = {3:'3h', 6:'6h', 12:'12h', 24:'1d', 24*2:'2d', 24*3:'3d', 24*7:'7d', 24*15:'15d', 24*30:'30d'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ LOAD DATA FROM DATABASE ------------------ #\n",
    "\n",
    "# ONE CRYPTO\n",
    "#df = alg_preproc.get_global_dataset_for_crypto(\"236131\")\n",
    "#df2 = df.reset_index(drop=True)\n",
    "\n",
    "# TOP N CRYPTOS\n",
    "#dict_df = alg_preproc.get_global_datasets_for_top_n_cryptos(2)\n",
    "\n",
    "# ------------------ LOAD DATA FROM FILE ------------------ #\n",
    "#save_obj(dict_df, 'dict_df_2018_10_26')\n",
    "#dict_df = load_obj('dict_df_2018_10_26') # top 30-40 \n",
    "#dict_df = load_obj('dict_df_2018_10_31') # top 10 \n",
    "#dict_df = load_obj('dict_df_2018_11_07') # top 10 \n",
    "#dict_df = load_obj('dict_df_2018_10_31_top2') # top 2\n",
    "\n",
    "dict_df = load_obj('dict_df_2018_11_13') # top 30\n",
    "#dict_df.keys()\n",
    "X_train, X_test, y_train, y_test = alg_preproc.get_preprocessed_data(dict_df, dict_hours_labels, \n",
    "                                                                     close_price_increase_targeted, \n",
    "                                                                     predict_only_one_crypto,\n",
    "                                                                     do_scale=True, \n",
    "                                                                     do_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion)\n",
    "confusion[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ALGO : RANDOM FOREST WITH RANDOMGRIDSEARCH\n",
    "# -----------------------\n",
    "\n",
    "rf_best_estimator, rf_best_params = alg_gridsearch.start_randomizedSearchCV_random_forest(X_train_scaled, \n",
    "                                                                                          y_train[y_to_be_considered])\n",
    "\n",
    "show_model_accuracy('RandomForestClassifier - Train', rf_best_estimator, X_train_scaled, y_train[y_to_be_considered], X_train_scaled.columns, do_precision_recall_curve=True, do_features_importance=True, threshold=threshold)\n",
    "show_model_accuracy('RandomForestClassifier - Test', rf_best_estimator, X_test_scaled, y_test[y_to_be_considered], X_test_scaled.columns, do_precision_recall_curve=True, do_features_importance=False, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ALGO : MLPC WITH RANDOMGRIDSEARCH\n",
    "# -----------------------\n",
    "\n",
    "mlpc_best_estimator, mlpc_best_params = alg_gridsearch.start_randomizedSearchCV_mlpc(X_train_scaled, \n",
    "                                                                                     y_train[y_to_be_considered])\n",
    "\n",
    "show_model_accuracy('RandomForestClassifier - Train', mlpc_best_estimator, X_train_scaled, y_train[y_to_be_considered], X_train_scaled.columns, do_precision_recall_curve=True, do_features_importance=False, threshold=threshold)\n",
    "show_model_accuracy('RandomForestClassifier - Test', mlpc_best_estimator, X_test_scaled, y_test[y_to_be_considered], X_test_scaled.columns, do_precision_recall_curve=True, do_features_importance=False, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ALGO : XGBoost WITH RANDOMGRIDSEARCH\n",
    "# -----------------------\n",
    "\n",
    "xgboost_best_estimator, xgboost_best_params = alg_gridsearch.start_randomizedSearchCV_xgboost(X_train, \n",
    "                                                                                           y_train[y_to_be_considered])\n",
    "\n",
    "show_model_accuracy('GradientBoostingClassifier - Train', xgboost_best_estimator, X_train, y_train[y_to_be_considered], None, do_precision_recall_curve=True, do_features_importance=False, threshold=threshold)\n",
    "show_model_accuracy('GradientBoostingClassifier - Test', xgboost_best_estimator, X_test, y_test[y_to_be_considered], None, do_precision_recall_curve=True, do_features_importance=False, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ALGO : RANDOM FOREST\n",
    "# -----------------------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#, n_estimators=100, max_features='auto', max_depth=5, criterion='entropy', class_weight=\"balanced\"\n",
    "clf = RandomForestClassifier(random_state=0, class_weight=\"balanced\", max_depth=5).fit(X_train, y_train[y_to_be_considered])\n",
    "\n",
    "show_model_accuracy('RandomForestClassifier - Train', clf, X_train, y_train[y_to_be_considered], None, do_precision_recall_curve=True, do_features_importance=True, threshold=threshold)\n",
    "show_model_accuracy('RandomForestClassifier - Test', clf, X_test, y_test[y_to_be_considered], None, do_precision_recall_curve=True, do_features_importance=False, threshold=threshold)\n",
    "\n",
    "# proba : to be shown by prediction, bien comprendre !\n",
    "probas = clf.predict_proba(X_test)[:,1]\n",
    "pprint(np.sort(probas))\n",
    "pprint(clf.get_params())\n",
    "\n",
    "# clf.decision_path(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ALGO : MLPClassifier - Neural Network (cf. Module+4 for tuning)\n",
    "# -----------------------\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nnclf = MLPClassifier(hidden_layer_sizes = [130, 130], solver='lbfgs',\n",
    "                     random_state = 0).fit(X_train, y_train[y_to_be_considered])\n",
    "\n",
    "show_model_accuracy('MLPClassifier - Neural Network - Train', nnclf, X_train, y_train[y_to_be_considered], None, do_roc_curve=True, do_precision_recall_curve=True, do_precision_recall_vs_treshold=True, do_features_importance=False, threshold=threshold)\n",
    "show_model_accuracy('MLPClassifier - Neural Network - Test', nnclf, X_test, y_test[y_to_be_considered], None, do_roc_curve=True, do_precision_recall_curve=True, do_precision_recall_vs_treshold=True, do_features_importance=False, threshold=threshold)\n",
    "\n",
    "# proba : to be shown by prediction, bien comprendre !\n",
    "#print(pd.DataFrame(nnclf.predict_proba(X_test_scaled)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# ALGO : XGBOOST (cf. Module+4 for tuning)\n",
    "# -----------------------\n",
    "from xgboost import XGBClassifier\n",
    "# Generally, the Scale_pos_weight is the ratio of number of negative class to the positive class. \n",
    "# Suppose, the dataset has 90 observations of negative class and 10 observations of positive class, \n",
    "# then ideal value of scale_pos_Weight should be 9. You can check the following link. \n",
    "# http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "xgbc = XGBClassifier().fit(X_train, y_train[y_to_be_considered])\n",
    "\n",
    "show_model_accuracy('XGBClassifier - Train', xgbc, X_train, y_train[y_to_be_considered], pX_columns=None, do_roc_curve=True, do_precision_recall_curve=True, do_precision_recall_vs_treshold=True, do_features_importance=False, threshold=threshold)\n",
    "show_model_accuracy('XGBClassifier - Test', xgbc, X_test, y_test[y_to_be_considered], pX_columns=None, do_roc_curve=True, do_precision_recall_curve=True, do_precision_recall_vs_treshold=True, do_features_importance=True, threshold=threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
