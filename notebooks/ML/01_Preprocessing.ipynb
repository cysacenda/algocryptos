{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas :\n",
    "    # Do not delete data from end / begining of dataset because of missing data (ex : 30 days EMA only available at row 30). \n",
    "        # => Do it at the end if extrapolation is not applicable\n",
    "    # Import old prices for crypto (even if it's only close_price to calcul indicators) ? => Avoid deleting data\n",
    "    \n",
    "    # When data importation OK, do data vizualisation to improve features choice / creation\n",
    "    \n",
    "    # Do interpolation / extrapolation and ensure that all data are coherents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :\n",
    "    # Add missing prices / volumes (+ extrapolate) to dataset for indicators calculs\n",
    "        # => Finish get_ohlcv_1d_plus_missing_infos() => dataset to be joined (some things to do before :p)\n",
    "    # Data extrapolation for columns, other things to have a very clean dataset ?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_csa import show_model_accuracy, remove_outliers\n",
    "\n",
    "import numpy as np\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from pytz import timezone\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import talib # https://github.com/mrjbq7/ta-lib    -    https://mrjbq7.github.io/ta-lib/\n",
    "from talib.abstract import *\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "ohlcv_columns_to_be_cleaned = ['close_price', 'open_price', 'low_price', 'high_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_timestamp_tasks(df_ts):\n",
    "    df_ts = df_ts[~df_ts.timestamp.duplicated(keep='first')]\n",
    "    df_ts['timestamp'] = pd.to_datetime(df_ts.timestamp, utc=True)\n",
    "    return df_ts.set_index('timestamp')\n",
    "\n",
    "def get_dataset_ohlcv(connection, id_cryptocompare):  \n",
    "    squery = \"select oh.open_price, oh.high_price, oh.low_price, oh.close_price, oh.volume_aggregated as volume_aggregated_1h, oh.timestamp\\n\" #re.reddit_subscribers,\n",
    "    squery += 'from histo_ohlcv oh\\n'\n",
    "    squery += 'where oh.id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += 'order by oh.timestamp asc\\n'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_reddit(connection, id_cryptocompare):\n",
    "    squery = \"select re.reddit_subscribers, date_trunc('day', re.timestamp) + '00:00:00' as timestamp\\n\"\n",
    "    squery += 'from social_stats_reddit_histo re\\n'\n",
    "    squery += 'where re.id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += 'order by re.timestamp asc\\n'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_all_cryptos(connection):\n",
    "    squery = 'select sum(hi.close_price * hi.volume_aggregated) as global_volume_usd_1h, sum(hi.close_price * pr.available_supply) as global_market_cap_usd, hi.timestamp\\n'\n",
    "    squery += 'from histo_ohlcv hi\\n'\n",
    "    squery += 'inner join coins co on (hi.id_cryptocompare = co.id_cryptocompare)\\n'\n",
    "    squery += 'left outer join prices pr on (pr.id_cryptocompare = hi.id_cryptocompare)\\n'\n",
    "    squery += 'group by timestamp\\n'\n",
    "    squery += 'order by timestamp'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_google_trend(connection, id_cryptocompare, period):\n",
    "    squery = 'select value_standalone, value_compared_to_standard, timestamp\\n'\n",
    "    squery += 'from social_google_trend' + period + '\\n'\n",
    "    squery += 'where id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += 'order by timestamp'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_dataset_ohlcv_old(connection, id_cryptocompare, before_date):\n",
    "    squery = \"select oh.open_price, oh.high_price, oh.low_price, oh.close_price, oh.volume_usd, oh.timestamp\\n\"\n",
    "    squery += 'from histo_ohlcv_old oh\\n'\n",
    "    squery += 'where oh.id_cryptocompare = ' + id_cryptocompare + '\\n'\n",
    "    squery += \"and oh.timestamp < '\" + str(before_date) + \"'\\n\"\n",
    "    squery += 'order by oh.timestamp desc\\n'\n",
    "    squery += 'limit 60\\n'\n",
    "    return psql.read_sql_query(squery, connection)\n",
    "\n",
    "def get_ohlcv_1d_plus_missing_infos(df_ohlcv_p, id_cryptocompare):    \n",
    "    df_ohlcv_old = get_dataset_ohlcv_old(connection, id_cryptocompare, df_ohlcv_p.index.min())\n",
    "    df_ohlcv_old = clean_dataset_ohlcv_std(df_ohlcv_old, ohlcv_columns_to_be_cleaned + ['volume_usd'], resample='1D')\n",
    "\n",
    "    # resample to 1d\n",
    "    df_ohlcv_1d = df_ohlcv_p.resample(\"1D\").agg({'open_price': 'first', 'high_price': 'max', 'low_price': 'min', \n",
    "                                     'close_price': 'last', 'volume_aggregated_1h': 'sum'})\n",
    "    \n",
    "    # TODO : mixing both dataset\n",
    "    \n",
    "    return df_ohlcv_1d\n",
    "\n",
    "def clean_dataset_google_trend(df_google_trend_p):\n",
    "    df_google_trend_p = do_timestamp_tasks(df_google_trend_p)\n",
    "    df_google_trend_p = df_google_trend_p.resample('1H').interpolate()\n",
    "    df_google_trend_p['value_standalone'] = df_google_trend_p['value_standalone'].astype(int)\n",
    "    df_google_trend_p['value_compared_to_standard'] = df_google_trend_p['value_compared_to_standard'].astype(int)\n",
    "\n",
    "    return df_google_trend_p\n",
    "\n",
    "def clean_dataset_ohlcv_spe(df_ohlcv_p):\n",
    "    # drop rows with missing values (OHLCV)\n",
    "    df_ohlcv_p = df_ohlcv_p.loc[(df_ohlcv_p.open_price != 0.0) & (df_ohlcv_p.high_price != 0.0) & (df_ohlcv_p.low_price != 0.0) & (df_ohlcv_p.close_price != 0.0) & (df_ohlcv_p.volume_aggregated_1h != 0.0)]\n",
    "\n",
    "    # TODO : KO, pas uniquement close_price...\n",
    "    return clean_dataset_ohlcv_std(df_ohlcv_p, ohlcv_columns_to_be_cleaned + ['volume_aggregated_1h'])\n",
    "\n",
    "def clean_dataset_ohlcv_std(df_ohlcv_p, columns_name, do_ts_tasks=True, resample='1H'):\n",
    "    # perform different tasks on df\n",
    "    if do_ts_tasks:\n",
    "        df_ohlcv_p = do_timestamp_tasks(df_ohlcv_p)\n",
    "    df_ohlcv_p = remove_outliers(df_ohlcv_p, columns_name)\n",
    "    \n",
    "    df_ohlcv_p = df_ohlcv_p.resample(resample).interpolate()\n",
    "    #print('shape after interpolate : ' + str(df_ohlcv.shape))\n",
    "    \n",
    "    return df_ohlcv_p\n",
    "\n",
    "def feature_engineering_ohlcv(df_ohlcv_p):\n",
    "    df_ohlcv_p = df_ohlcv_p.copy()\n",
    "    \n",
    "    # volume_aggregated_24h\n",
    "    df_ohlcv_p['volume_aggregated_24h'] = df_ohlcv_p.volume_aggregated_1h.rolling(24).sum()\n",
    "    \n",
    "    # close price variance on different scales\n",
    "    df_ohlcv_p['close_price_variance_3h'] = df_ohlcv_p.close_price.rolling(3).var()\n",
    "    df_ohlcv_p['close_price_variance_12h'] = df_ohlcv_p.close_price.rolling(12).var()\n",
    "    df_ohlcv_p['close_price_variance_24h'] = df_ohlcv_p.close_price.rolling(24).var()\n",
    "    df_ohlcv_p['close_price_variance_7d'] = df_ohlcv_p.close_price.rolling(24 * 7).var()\n",
    "    df_ohlcv_p['close_price_variance_15d'] = df_ohlcv_p.close_price.rolling(24 * 15).var()\n",
    "    df_ohlcv_p['close_price_variance_30d'] = df_ohlcv_p.close_price.rolling(24 * 30).var()\n",
    "    \n",
    "    # variance high / low on period\n",
    "    df_ohlcv_p['last_period_high_low_price_var_pct'] = abs(df_ohlcv_p['low_price'] - df_ohlcv_p['high_price']) / df_ohlcv_p['close_price']\n",
    "    \n",
    "    # volumes kpis 1h, 3h, 6h, 12h, 24h, 3d, 7d, 15d\n",
    "    df_ohlcv_p['mean_volume_1h_30d'] = df_ohlcv_p.volume_aggregated_1h / df_ohlcv_p.volume_aggregated_1h.rolling(30 * 24).mean()\n",
    "    arr_nums = [3, 6, 12, 24, 3 * 24, 7 * 24, 15 * 24]\n",
    "    arr_labels = ['3h', '6h', '12h', '24h', '3d', '7d', '15d']\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_ohlcv_p['mean_volume_' + arr_labels[i] + '_30d'] = df_ohlcv_p.volume_aggregated_1h.rolling(arr_nums[i]).mean() / df_ohlcv_p.volume_aggregated_1h.rolling(30 * 24).mean()\n",
    "    \n",
    "    # change vs n days low / n days high - pct_change for periods : 3d, 7d, 15d, 30d\n",
    "    arr_nums = np.array([3, 7, 15, 30], dtype=int) * 24\n",
    "    arr_labels = ['3d', '7d', '15d', '30d']\n",
    "    \n",
    "    # lows\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_ohlcv_p['close_price_pct_change_vs_' + arr_labels[i] + '_low'] = (df_ohlcv_p.close_price - df_ohlcv_p.close_price.rolling(arr_nums[i]).min()) / df_ohlcv_p.close_price.rolling(arr_nums[i]).min()      \n",
    "        \n",
    "    # highs\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_ohlcv_p['close_price_pct_change_vs_' + arr_labels[i] + '_high'] = (df_ohlcv_p.close_price - df_ohlcv_p.close_price.rolling(arr_nums[i]).max()) / df_ohlcv_p.close_price.rolling(arr_nums[i]).max()      \n",
    "   \n",
    "    return df_ohlcv_p\n",
    "\n",
    "def feature_engineering_ohlcv_all_cryptos(df_ohlcv_all_p):\n",
    "    # volume_aggregated_24h\n",
    "    df_ohlcv_all_p['global_volume_usd_24h'] = df_ohlcv_all_p.global_volume_usd_1h.rolling(24).sum()\n",
    "    \n",
    "    return df_ohlcv_all_p\n",
    "\n",
    "def feature_engineering_reddit(df_reddit_p):    \n",
    "    # pct_change for periods : 1d, 3d, 7d, 15d, 30d\n",
    "    arr_nums = np.array([1, 3, 7, 15, 30], dtype=int) * 24\n",
    "    arr_labels = ['1d', '3d', '7d', '15d', '30d']\n",
    "    for i in range(len(arr_nums)):\n",
    "        df_reddit['reddit_subscribers_pct_change_' + arr_labels[i]] = df_reddit.reddit_subscribers.pct_change(periods=arr_nums[i])\n",
    "    \n",
    "    return df_reddit_p\n",
    "\n",
    "def feature_engineering_google_trend(df_google_trend_p, period):\n",
    "    # period = month\n",
    "    arr_nums = np.array([1, 3, 7, 15, 30], dtype=int) * 24\n",
    "    arr_labels = ['1d', '3d', '7d', '15d', '30d']\n",
    "    \n",
    "    #period = year\n",
    "    if period == 'y':\n",
    "        # pct_change for periods : 2m, 3m, 6m, 1y\n",
    "        arr_nums = np.array([2, 3, 6, 12], dtype=int) * 24 * 30\n",
    "        arr_labels = ['2m', '3m', '6m', '1y']   \n",
    "    \n",
    "    for i in range(len(arr_nums)):\n",
    "        df_google_trend_p['gg_trend_value_standalone_pct_change_' + arr_labels[i]] = df_google_trend_p.value_standalone.pct_change(periods=arr_nums[i])\n",
    "        df_google_trend_p['gg_trend_value_compared_pct_change_' + arr_labels[i]] = df_google_trend_p.value_compared_to_standard.pct_change(periods=arr_nums[i])\n",
    "    return df_google_trend_p\n",
    "\n",
    "def feature_engineering_technical_analysis(df_ohlcv_p, df_ohlcv_1d):\n",
    "    # TODO : + Add slopes for all (differents slopes for Overlap indicators) + boolean for threshold (ex : RSI 30 / 70)\n",
    "    df_ohlcv_tmp = df_ohlcv_p\n",
    "    \n",
    "    \n",
    "\n",
    "    # [Overlap Studies] EMA 30 days\n",
    "    df_ohlcv_1d['Indic_EMA_30d'] = EMA(df_ohlcv_1d, price='close_price', timeperiod=30)\n",
    "    \n",
    "    # [Interpretation] EMA 30 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_EMA_30d_uptrend'] = df_ohlcv_1d.Indic_EMA_30d.pct_change(periods=1) > 0\n",
    "    \n",
    "    # [Overlap Studies] EMA 15 days\n",
    "    df_ohlcv_1d['Indic_EMA_15d'] = EMA(df_ohlcv_1d, price='close_price', timeperiod=15)\n",
    "    \n",
    "    # [Interpretation] EMA 15 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_EMA_15d_uptrend'] = df_ohlcv_1d.Indic_EMA_15d.pct_change(periods=1) > 0\n",
    "    \n",
    "    # [Overlap Studies] EMA 7 days\n",
    "    df_ohlcv_1d['Indic_EMA_7d'] = EMA(df_ohlcv_1d, price='close_price', timeperiod=7)\n",
    "    \n",
    "    # [Interpretation] EMA 7 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_EMA_7d_uptrend'] = df_ohlcv_1d.Indic_EMA_7d.pct_change(periods=1) > 0\n",
    "    \n",
    "    \n",
    "\n",
    "    # [Overlap Studies] MA 30 days\n",
    "    df_ohlcv_1d['Indic_MA_30d'] = MA(df_ohlcv_1d, price='close_price', timeperiod=30, matype=0)\n",
    "    \n",
    "    # [Interpretation] MA 30 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_MA_30d_uptrend'] = df_ohlcv_1d.Indic_MA_30d.pct_change(periods=1) > 0\n",
    "    \n",
    "    # [Overlap Studies] MA 15 days\n",
    "    df_ohlcv_1d['Indic_MA_15d'] = MA(df_ohlcv_1d, price='close_price', timeperiod=15, matype=0)\n",
    "    \n",
    "    # [Interpretation] MA 15 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_MA_15d_uptrend'] = df_ohlcv_1d.Indic_MA_15d.pct_change(periods=1) > 0\n",
    "    \n",
    "    # [Overlap Studies] MA 7 days\n",
    "    df_ohlcv_1d['Indic_MA_7d'] = MA(df_ohlcv_1d, price='close_price', timeperiod=7, matype=0)\n",
    "    \n",
    "    # [Interpretation] MA 7 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_MA_7d_uptrend'] = df_ohlcv_1d.Indic_MA_7d.pct_change(periods=1) > 0\n",
    "    \n",
    "    \n",
    "\n",
    "    # [Overlap Studies] BBands - TODO : 20 days ?\n",
    "    bands = BBANDS(df_ohlcv_1d, price='close_price', timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    bands.columns = ['Indic_Bbands_20d_upperband', 'Indic_Bbands_20d_middleband', 'Indic_Bbands_20d_lowerband']\n",
    "    df_ohlcv_1d = df_ohlcv_1d.join(bands)\n",
    "    \n",
    "    # [Interpretation] BBands close_price - Indic_Bbands_20d_upperband\n",
    "    df_ohlcv_1d['Indic_Bbands_20d_diff_close_upperband'] = df_ohlcv_1d.close_price - df_ohlcv_1d.Indic_Bbands_20d_upperband\n",
    "    \n",
    "    # [Interpretation] BBands close_price - Indic_Bbands_20d_middleband\n",
    "    df_ohlcv_1d['Indic_Bbands_20d_diff_close_upperband'] = df_ohlcv_1d.close_price - df_ohlcv_1d.Indic_Bbands_20d_middleband\n",
    "    \n",
    "    # [Interpretation] BBands close_price - Indic_Bbands_20d_middleband\n",
    "    df_ohlcv_1d['Indic_Bbands_20d_diff_close_lowerband'] = df_ohlcv_1d.close_price - df_ohlcv_1d.Indic_Bbands_20d_lowerband\n",
    "    \n",
    "    # TODO / Ideas : Boolean > 0 < 0 ?\n",
    "    \n",
    "\n",
    "    # [Momentum Indicator] RSI 14 days\n",
    "    df_ohlcv_1d['Indic_RSI_14d'] = RSI(df_ohlcv_1d, price='close_price', timeperiod=14)\n",
    "    \n",
    "    # [Interpretation] RSI 14 days in uptrend : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_RSI_14d_uptrend'] = df_ohlcv_1d.Indic_RSI_14d.pct_change(periods=1) > 0\n",
    "    \n",
    "    # [Interpretation] RSI 14 days > value 70\n",
    "    df_ohlcv_1d['Indic_RSI_sup_70'] = df_ohlcv_1d.Indic_RSI_14d > 70\n",
    "    \n",
    "    # [Interpretation] RSI 14 days < value 30\n",
    "    df_ohlcv_1d['Indic_RSI_inf_30'] = df_ohlcv_1d.Indic_RSI_14d < 30\n",
    "    \n",
    "    \n",
    "\n",
    "    # [Momentum Indicators] STOCH\n",
    "    # ta-lib abstract API KO with dataframe : use workaround\n",
    "    dataset = {'high': df_ohlcv_1d.high_price.values, 'low': df_ohlcv_1d.low_price.values, 'close': df_ohlcv_1d.close_price.values}\n",
    "    kd = STOCH(dataset, fastk_period=14, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "    df_ohlcv_1d['Indic_Stoch_14_3_3_k'] = kd[0]\n",
    "    df_ohlcv_1d['Indic_Stoch_14_3_3_d'] = kd[1]\n",
    "    \n",
    "    # [Interpretation] STOCH > value 80\n",
    "    df_ohlcv_1d['Indic_Stoch_14_3_3_sup_80'] = (df_ohlcv_1d.Indic_Stoch_14_3_3_k > 80) & (df_ohlcv_1d.Indic_Stoch_14_3_3_d > 80)\n",
    "    \n",
    "    # [Interpretation] STOCH < value 20\n",
    "    df_ohlcv_1d['Indic_Stoch_14_3_3_inf_20'] = (df_ohlcv_1d.Indic_Stoch_14_3_3_k < 20) & (df_ohlcv_1d.Indic_Stoch_14_3_3_d < 20)\n",
    "    \n",
    "    # [Interpretation] STOCH diff\n",
    "    df_ohlcv_1d['Indic_Stoch_14_3_3_diff'] = df_ohlcv_1d.Indic_Stoch_14_3_3_k - df_ohlcv_1d.Indic_Stoch_14_3_3_d\n",
    "    \n",
    "    \n",
    "\n",
    "    # [Momentum Indicators] MACD\n",
    "    macd = MACD(df_ohlcv_1d, price='close_price', fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    macd.columns = ['Indic_Macd_12_26_9_macd', 'Indic_Macd_12_26_9_macdsignal', 'Indic_Macd_12_26_9_macdhist']\n",
    "    df_ohlcv_1d = df_ohlcv_1d.join(macd)\n",
    "    \n",
    "    # TODO / Ideas : ?\n",
    "    \n",
    "    \n",
    "\n",
    "    # [Volume Indicators] OBV\n",
    "    dataset = {'close': df_ohlcv_1d.close_price.values, 'volume': df_ohlcv_1d.volume_aggregated_1h.values}\n",
    "    obv = OBV(dataset)\n",
    "    df_ohlcv_1d['Indic_OBV'] = obv\n",
    "    \n",
    "    # [Interpretation] OBV in uptrend on last 3d : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_OBV_uptrend_3d'] = df_ohlcv_1d.Indic_OBV.pct_change(periods=3) > 0\n",
    "    \n",
    "    # [Interpretation] OBV in uptrend on last 7d : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_OBV_uptrend_7d'] = df_ohlcv_1d.Indic_OBV.pct_change(periods=7) > 0\n",
    "    \n",
    "    # [Interpretation] OBV in uptrend on last 15d : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_OBV_uptrend_15d'] = df_ohlcv_1d.Indic_OBV.pct_change(periods=15) > 0\n",
    "    \n",
    "    # [Interpretation] OBV in uptrend on last 30d : True / downtrend : False\n",
    "    df_ohlcv_1d['Indic_OBV_uptrend_30d'] = df_ohlcv_1d.Indic_OBV.pct_change(periods=30) > 0\n",
    "    \n",
    "        \n",
    "\n",
    "    # join to have all indicators in one df\n",
    "    df_ohlcv_1d = df_ohlcv_1d.drop(['open_price', 'high_price', 'low_price', 'close_price', 'volume_aggregated_1h'], axis=1)\n",
    "    df_ohlcv_tmp = df_ohlcv_tmp.join(df_ohlcv_1d.resample('1H').interpolate())\n",
    "    \n",
    "    return df_ohlcv_tmp.drop(['open_price', 'high_price', 'low_price', 'close_price', 'volume_aggregated_1h'], axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ PRE-PROCESSING : Data retrieving & cleaning ------------------ #\n",
    "\n",
    "# TODO : Replace with info from config file\n",
    "connection = create_engine('postgresql://dbuser:algocryptos@localhost:9091/algocryptos')\n",
    "\n",
    "# Crypto ids\n",
    "id_cryptocompare_crypto = \"7605\"\n",
    "id_cryptocompare_tether = \"171986\"\n",
    "id_cryptocompare_bitcoin = \"1182\"\n",
    "\n",
    "# --------------------------------\n",
    "# OHLCV\n",
    "# --------------------------------\n",
    "df_ohlcv = get_dataset_ohlcv(connection, id_cryptocompare_crypto)\n",
    "df_ohlcv = clean_dataset_ohlcv_spe(df_ohlcv)\n",
    "\n",
    "df_ohlcv_tether = get_dataset_ohlcv(connection, id_cryptocompare_tether)\n",
    "df_ohlcv_tether = clean_dataset_ohlcv_spe(df_ohlcv_tether)\n",
    "\n",
    "df_ohlcv_bitcoin = get_dataset_ohlcv(connection, id_cryptocompare_bitcoin)\n",
    "df_ohlcv_bitcoin = clean_dataset_ohlcv_spe(df_ohlcv_bitcoin)\n",
    "\n",
    "df_ohlcv_1d = get_ohlcv_1d_plus_missing_infos(df_ohlcv, id_cryptocompare_crypto)\n",
    "\n",
    "# --------------------------------\n",
    "# REDDIT SUBSCRIBERS\n",
    "# --------------------------------\n",
    "# /!\\ TODO : Mauvaise extrapolation sur derniÃ¨res heures du nb de subscribers => function qui extrapole n colonnes\n",
    "df_reddit = get_dataset_reddit(connection, id_cryptocompare_crypto)\n",
    "df_reddit = df_reddit[df_reddit.reddit_subscribers.notnull()]\n",
    "df_reddit = do_timestamp_tasks(df_reddit)\n",
    "df_reddit = df_reddit.resample('1H').interpolate()\n",
    "df_reddit['reddit_subscribers'] = df_reddit['reddit_subscribers'].astype(int)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# ALL CRYPTOS\n",
    "# --------------------------------\n",
    "df_all_cryptos = get_dataset_all_cryptos(connection)\n",
    "df_all_cryptos = clean_dataset_ohlcv_std(df_all_cryptos, columns_name=['global_volume_usd_1h', 'global_market_cap_usd'])\n",
    "#df_all_cryptos = clean_dataset_ohlcv_std(df_all_cryptos, columns_name=['global_market_cap_usd'], do_ts_tasks=False)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# GOOGLE TREND\n",
    "# --------------------------------\n",
    "# crypto - last month\n",
    "df_google_trend_crypto_1m = get_dataset_google_trend(connection, id_cryptocompare_crypto, '_1m')\n",
    "df_google_trend_crypto_1m = clean_dataset_google_trend(df_google_trend_crypto_1m)\n",
    "\n",
    "# crypto - 5 years\n",
    "df_google_trend_crypto_5y = get_dataset_google_trend(connection, id_cryptocompare_crypto, '')\n",
    "df_google_trend_crypto_5y = clean_dataset_google_trend(df_google_trend_crypto_5y)\n",
    "\n",
    "# bitcoin - last month\n",
    "df_google_trend_bitcoin_1m = get_dataset_google_trend(connection, id_cryptocompare_bitcoin, '_1m')\n",
    "df_google_trend_bitcoin_1m = clean_dataset_google_trend(df_google_trend_bitcoin_1m)\n",
    "\n",
    "# bitcoin - 5 years\n",
    "df_google_trend_bitcoin_5y = get_dataset_google_trend(connection, id_cryptocompare_bitcoin, '')\n",
    "df_google_trend_bitcoin_5y = clean_dataset_google_trend(df_google_trend_bitcoin_5y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure\n",
    "fig1 = plt.figure(figsize=(15,15))\n",
    "#df_ohlcv.close_price.plot()\n",
    "#df2.volume_aggregated.plot(secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ PRE-PROCESSING : Feature engineering ------------------ #\n",
    "df_reddit = feature_engineering_reddit(df_reddit)\n",
    "df_ohlcv_fe = feature_engineering_ohlcv(df_ohlcv)\n",
    "df_ohlcv_tether_fe = feature_engineering_ohlcv(df_ohlcv_tether)\n",
    "df_ohlcv_bitcoin_fe = feature_engineering_ohlcv(df_ohlcv_bitcoin)\n",
    "df_technical_analysis = feature_engineering_technical_analysis(df_ohlcv, df_ohlcv_1d)\n",
    "df_all_cryptos = feature_engineering_ohlcv_all_cryptos(df_all_cryptos)\n",
    "df_google_trend_crypto_1m = feature_engineering_google_trend(df_google_trend_crypto_1m, 'm')\n",
    "df_google_trend_bitcoin_1m = feature_engineering_google_trend(df_google_trend_bitcoin_1m, 'm')\n",
    "df_google_trend_crypto_5y = feature_engineering_google_trend(df_google_trend_crypto_5y, 'y')\n",
    "df_google_trend_bitcoin_5y = feature_engineering_google_trend(df_google_trend_bitcoin_5y, 'y')\n",
    "\n",
    "# Join dfs\n",
    "#df_ohlcv = df_ohlcv.join(df_ohlcv_tether[['close_price','volume_aggregated_1h']], rsuffix='_tether') => Subset only\n",
    "df_ohlcv_fe = df_ohlcv_fe.join(df_ohlcv_tether_fe, rsuffix='_tether')\n",
    "df_ohlcv_fe = df_ohlcv_fe.join(df_ohlcv_bitcoin_fe, rsuffix='_bitcoin')\n",
    "\n",
    "df_global = df_ohlcv_fe.join(df_technical_analysis)\n",
    "df_global = df_global.join(df_reddit)\n",
    "df_global = df_global.join(df_all_cryptos)\n",
    "df_global = df_global.join(df_google_trend_crypto_1m, rsuffix='_crypto_1m')\n",
    "df_global = df_global.join(df_google_trend_bitcoin_1m, rsuffix='_bitcoin_1m')\n",
    "df_global = df_global.join(df_google_trend_crypto_5y, rsuffix='_crypto_5y')\n",
    "df_global = df_global.join(df_google_trend_bitcoin_5y, rsuffix='_bitcoin_5y')\n",
    "df_global.resample('1H').interpolate()\n",
    "df_global.reddit_subscribers = df_global.reddit_subscribers.interpolate(method='linear', limit_area='outside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure\n",
    "fig1 = plt.figure(figsize=(15,15))\n",
    "#df_global.Indic_MA_30d.plot(label='close_price', color='black')\n",
    "#df_global.Indic_MA_30d.plot(secondary_y=True, color='red')\n",
    "\n",
    "#df_ohlcv_1d.Indic_Macd_12_26_9_macdsignal.plot(secondary_y=True, color='blue')\n",
    "#df_ohlcv_1d.Indic_Macd_12_26_9_macdhist.plot(secondary_y=True, color='red')\n",
    "\n",
    "#df_ohlcv_1d.close_price.plot()\n",
    "#df_ohlcv_1d.Indic_OBV.plot(secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
